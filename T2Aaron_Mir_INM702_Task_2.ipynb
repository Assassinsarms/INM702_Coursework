{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.1 64-bit",
   "display_name": "Python 3.8.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# INM702 Coursework Task 2\n",
    "### Aaron Mir (Student Number: 160001207)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"Task2Overview.jpg\" alt=\"Task2Overview\" width=\"700\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The second task is about classifying handwritten digits. We will use the MNIST dataset for training and testing. The point of this task is to develop a multi-layer neural network for classification using mostly Numpy:\n",
    "\n",
    "\n",
    "    • Implement sigmoid and relu layers (with forward and backward pass)\n",
    "\n",
    "    • Implement a softmax output layer\n",
    "    \n",
    "    • Implement a fully parameterizable neural network (number and types of layers, number of units)\n",
    "\n",
    "    • Implement an optimizer(e.g. SGD or Adam)and a stopping criterion of your choosing\n",
    "\n",
    "    • Train your Neural Network using backpropagation\n",
    "\n",
    "Evaluate different neural network architectures and compare your different results. e.g. fully relu vs fully sigmoid, number of layers etc.\n",
    "\n",
    "You can also compare withthe results presented in http://yann.lecun.com/exdb/mnist/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}